\newcommand{\h}{$h(\vec{x})$}
\newcommand{\e}{\epsilon}
\chapter{Neural networks}\label{chapter:NN}
In modern experiments data sets have become extrimelly multi-dimensonal. Different detectors provides different kind of information, which has to be combined together. Additionally variables are mostly correlated, quite often in non-linear way. A high dimensonality and correlations makes any analysis a demanding task. To make it easier, a set of automatic machine-learning methods has been developed through last 30 years [ref]. Between them the most known are: decision trees (DT), boosted decision trees (BDT), support vector machines (SVM) and neural networks (NN). In contrary to classical hard-cuts, witch base on a user's knowledge about an examination subject, a machine learning bases on statistical properties of a data sets.

There are two main ways how to divide all machine learning methods. Firstly on account of a solved problems: classification or regression. Models from a first group have a task to assign an input to one of the pre-defined classes (eg. a picture recognition). Regression models have to predict output value base on input variables (eg. prediction of a flat prices base on its properties). A second division bases on way of training. In supervised learning, free parameters of a model are set using so-called learning sets. They are sets of data already labeled, it means that both: an input end expected output are known. An example can be a well-know linear regression. Using knowing points a user calculates a line coefficients. Next the obtained function can be used for predictions. An unsupervised learning bases only on an exterminated dataset properties, for example clustering algorithms. Of course such a simple classification do no describe all possible algorithms, they are ways of learning called: semi-supervised, active learning, reinforcement learning and others which do not fit unambiguously to one of mentioned categories. Nonetheless they have a minor significance for experimental particle physics.

Scope of the following work is restricted to supervised methods devoted to classification problems. They have an essential role in signal and background discrimination. 

\section{Work with neural networks}
In case of classification problems, regardless of a used model, a general workflow is very similar. A user has to prepare a set of labeled data points. The set has to be randomly divided into two independent subsets: a training set (sometimes called learning) and a testing set. It is very important to not mix those two sets, otherwise a model will learn only particular properties of the training set and will not generalize to other data. A model parameters are adjusted by in iterative way using the learning set. After the end of the learning process the training set is used to determine performance of the model. After whole training process the trended model can be used in physical analysis as a signal-background discriminator. Whole process is graphically described in fig. \ref{fig:data_flow}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1 \linewidth]{Chapter_NN/data_flow.eps}
  \caption{A data flow for training and testing of a machine-learning model. A red part of the diagram describe model training and testing. A greens part is devoted to using the model in physical application.}
  \label{fig:data_flow}
\end{figure}


\section{The ROC curve and the optimal classifier}
One of the most common problem in machine learning is a binary classification, when a data set has to be divided into two subsets, fulfilling certain requirements. A simple example of such a problem is distinction between signal and background events in data collected by experiment. We would like to have a function which takes as arguments set of physical observables (eg. particles' energy, momentum, coordinates of vertexes), represents by $\vec{x}$ and returns single number. More formally, a classifier can be call any function $h: \vec x \rightarrow \mathbb{R}$ designed in such a way, that high \h values correspond signal events and low \h values correspond background event. In most cases a classifier output is squeezed by activation function to some finite range, for example from 0 to 1. A threshold value  \h =c, which is the value separating signal and background events is called a working point, and has to be set by a user. The signal efficiency will be defined as $\e_S=\int d\vec x \rho_S(\vec x) \Theta(h(\vec x) -c)$ and respectively a background efficiency $\e_B=\int d\vec x \rho_B(\vec x) \Theta(h(\vec x) -c)$.

Problems: how to represent a classifier performance, how to compare different classifiers and how to choose a proper working point, have been discussed since many years. During World War II engineers faced a similar issue: a radar detection efficiency. With an increasing radar sensitivity a chance to detect an enemy aircraft increases. However, the chance that signal is a fake caused by birds or other circumstances also increases. To represent this relation a so-colled ROC (\textbf{R}eceiver \textbf{O}perating \textbf{C}haracteristic) curve was invited. One axis represents a true positive rate or a detection efficiency, second axis shows a background reduction. Each point on the curve represents working-point for the classifier. Comparing two different classifiers someone can not compare performance for one working point, but has to compare all. Graphic representation of compared classyficators can be given by ROC curves.

\begin{figure}[hb]
  \centering
  \includegraphics[width=0.7 \linewidth]{Chapter_NN/ROC.eps}
  \caption{Examples of a ROC curve. It represents a classifier performance. In case of the ideal classifier area under the curve is equal 1, what means that for each working point all background events are rejected and non of signal is lost. For a given example the most optimal results are given by multi layer perceptron (MLP) and boosted decision tree (BDT). The picture comes from \cite{TMVA}}
  \label{fig:ROC}
\end{figure}

The optimal classifier is deffined as
\begin{equation}
  \forall c \forall \e_S
\end{equation}

In graphical approach the optimal classifier should give the biggest area under the ROC curve from all possible classifiers. In case of completely separable sets of data the area should be equal 1.

\section{The data-driven approach}
The original paper by Metodiev, Nachman and Thaler \cite{Metodiev_2017} the authors show the idea of a data-driven analysis in details. In this chapter I want to introduce main concepts, necessary to understand how the proposed methodm helps in week decays reconstruction.

In a classical approach to supervized machine learning, a model learns its properties usign sets of labeled data. Of courese providing good training sets is always a problem. To do this, someone can use either experimental data, labeled by a user, or simulation. In first case a user uses his external knowledge about the data to describe it. In second case the user fully rely on simulation. First case is quite often impossible to perform, but even when a user is able to label data a labeling could be systematiclly biased by a lack of knowlage or missunderstand of detector. In second case they are two main threats: either a simulation does not describe data well, or creates some artificial structures in data, which can lead into sistematic errors in network performence. 

The data-data driven analysis avoids inconveniencees of two mentioned methodes. It requires neither labeling nor simulation and it bases only on statistical properties of a collected deta set. According to Neyman-Pearson lemma \cite{Neyman-Pearson} the optimal clasyfier for two sets, A and B is a function given by a dencity ratio
\begin{equation}
  \label{eq:N-P}
  h_{opt}^{A/B}(\vec x)=\frac{\rho_A}{\rho_B}
\end{equation}
or any monotonous function of $\frac{\rho_A}{\rho_B}$. Assuming that both sets A and B contains signal (s) and bacground (b) events and a statistical distribution of s and b is the same in A and B, we can write \eqref{eq:N-P} in the following way
\begin{equation}
  h^{A/B}_{opt}=\frac{f_1 \rho_s + (1-f_1) \rho_b}{f_2 \rho_s + (1-f_2) \rho_b}=\frac{f_1 \rho_s/\rho_b+1-f_1}{f_2 \rho_s/\rho_b+1-f_2}=\frac{f_1 h_{opt}^{s/b}+1-f_1}{f_2 h_{opt}^{s/b} +1-f_2}.
\end{equation}
It can be proven that $\partial_{h_{opt}^{s/b}}h_{opt}^{A/B} >0$, what means that optimal clasyfier for both cases is the same, so the clasyfier trained to distinguish A and B should also gives some separation power between signal and background. The situation is represented graphically in fig. \ref{fig:DD}.  It is important to underline that the reasoning gives no clue about the working points for both cases. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5 \textwidth]{Chapter_NN/setAB.eps}
\caption{A data-driven approach visualisation. According to \cite{Metodiev_2017} the opitimal clasyfier for sets A and B is equivalent to optimal clasyfier for sets s and b.}
\label{fig:DD}
\end{figure}



\section{Application for analysis}
In case of $\Ls$ reconstruction the data driven approach was used to replace set of geometrical cuts and enchace a $\Lz$ signal~to~background ratio. So for the neural network all events with $\Lz$ were treated as a signal and without like a background. For all events an invariant mas of $\p \pim$ pair was ploted (fig. \ref{fig:L1116_DD}). Using this spectrun the dataset was divided in two subsets: for $M^{inv}_{\p\pim} \in (1015,1125)$ and $M^{inv}_{\p\pim} \notin (1015,1125)$. In the first of them a ratio between $\Lz$ and background is clearly different than in the second, what fulfil the requariments for the data-driven aproach. Hence, a numerous networka architecture were tested to check which deals the best with $\Lz$ reconstruction.

\begin{figure}[ht]
  \includegraphics[width=0.8 \textwidth]{Chapter_NN/L1116.eps}
  \caption{A $\p \pim$ invariant mass specrum. Whole data set was divided into two subsets, A and B, each of them is characterized by different signal~to~background ratio. All tested models were trained to distinguish between sets A and B and later use to seperate events containing $\Lz$.}
  \label{fig:L1116_DD}
\end{figure}

A learning and testing proces was done within scope of TMVA framework \cite{TMVA}. Using TMVA a user has to provide list of input variables and a network architecture, than the framework automaticly preperes learning and testing sets and performs whole learning process together with testing the final classyfier. As an input variables the set of geometrical properties was used.
\begin{itemize}
\item Distance between $\p \pim$
\item $\Lz$ vertex cooridanes, reconstructed as a point of the closest aproach of $\p$ and $\pim$ tracks
\item $\Ls$ vertex coordinates, rseconstructed as a point of the closest approach of $\pip$ and $\pim$ tracks
\item $\Ls$ vertex coordinates, reconstructed by tracking algorithm as a primary vertex
\item Opaning angle between reconstructed $\Lz$ vector and a line conecting primary and secondary verteces
\end{itemize}
Using any combination of input variables it is impossile to reconstruct $\p \pim$ invariant mass. It is important feature wich allows to use mesioned invariant mass spectrum as a cross-check for all procedure, and a network will not collapse into trivial solution.

During treaning the network is optymalized to seperate sets A and B. Its performence for $\Lz$ reconstruction has to be investigated in a different way. After the traning the network was used to evaluate each event collected during the experiment. It means, that network output - a number in range from 0 to 1 - is assigned to each reconstructed event. Fitting a $\Lz$ peak (see \ref{fig:NN_wynik} a)) it was possible to check how signal efficiency and signal~to~background ratio changes with cut on the network output.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9 \textwidth]{Chapter_NN/NN_wyniki.eps}
  \caption{Results of a neural network training. a)Example of two spactra after a cut on the network output. For each of the the signal (gausian function) and background (4-th order polynomial) were fitted. Such fits allows to calculate a signal efficiency $\frac{S}{S_0}$ and a background rejection $1-\frac{B}{B_0}$, where $S_0$ and $B_0$ are yelds of signal and background without any cut on a neural network output. }
  \label{fig:NN_wynik}
\end{figure}

%%% Local Variables:
%%% TeX-master: "../main"
%%% End: 